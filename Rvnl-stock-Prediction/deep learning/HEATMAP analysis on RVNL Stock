import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# Dummy input: 1 sample with 60 time steps and 1 feature (e.g., closing price)
seq_len = 60
features = 1
x = torch.randn(1, seq_len, features)

# Self-attention layer (1 head for simplicity)
class TimeSeriesAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=1, batch_first=True)
        self.linear = nn.Linear(features, d_model)

    def forward(self, x):
        x_embed = self.linear(x)  # [batch, seq, d_model]
        attn_output, attn_weights = self.attn(x_embed, x_embed, x_embed, need_weights=True)
        return attn_output, attn_weights

# Initialize
model = TimeSeriesAttention(d_model=32)
attn_output, attn_weights = model(x)  # attn_weights shape: [1, seq_len, seq_len]

# Visualize attention for sample 0
attention = attn_weights[0].detach().numpy()

plt.figure(figsize=(10, 8))
plt.imshow(attention, cmap='viridis')
plt.colorbar()
plt.title("Attention Heatmap: Past 60 Days")
plt.xlabel("Day t - N")
plt.ylabel("Attending to")
plt.xticks(ticks=np.arange(seq_len), labels=[f"Day -{seq_len - i}" for i in range(seq_len)], rotation=90)
plt.yticks(ticks=np.arange(seq_len), labels=[f"Day -{seq_len - i}" for i in range(seq_len)])
plt.tight_layout()
plt.show()
